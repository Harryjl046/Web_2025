import os
import nltk
from pathlib import Path
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk import pos_tag
from tqdm import tqdm

def get_wordnet_pos(treebank_tag):
    """å°†è¯æ€§çš„Penn Treebank(è¿™ä»€ä¹ˆä¸œè¥¿)æ ‡è®°è½¬åŒ–ä¸ºWordNetæ ‡è®°"""
    if treebank_tag.startswith("J"):
        return wordnet.ADJ
    elif treebank_tag.startswith("V"):
        return wordnet.VERB
    elif treebank_tag.startswith("N"):
        return wordnet.NOUN
    elif treebank_tag.startswith("R"):
        return wordnet.ADV
    else:
        return wordnet.NOUN


BASE_DIR = Path(__file__).resolve().parents[2]
input_folder = BASE_DIR / "Lab1/descriptions"
output_folder = BASE_DIR / "Lab1/tokenized"
os.makedirs(output_folder, exist_ok=True)
NLTK_DIR = BASE_DIR / "Lab1/nltk_dir"
NLTK_DIR.mkdir(parents=True, exist_ok=True)     #è¿™æ˜¯æˆ‘è‡ªå®šä¹‰çš„nltkæ•°æ®å­˜æ”¾è·¯å¾„
#æˆ‘è‡ªå®šä¹‰äº†ä¸‹è½½åœ°å€æ‰€ä»¥éœ€è¦é‡æ–°åŠ è½½åœ°å€,å¦‚æœä½¿ç”¨é»˜è®¤ä¸‹è½½åœ°å€åˆ™ä¸éœ€è¦
nltk.data.path.append(str(NLTK_DIR)) 

# å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡ä½¿ç”¨ï¼Œéœ€è¦å…ˆä¸‹è½½ä¸€æ¬¡èµ„æºï¼Œå–æ¶ˆæ‰ä¸‹é¢çš„æ³¨é‡Š
'''
nltk.download('stopwords', download_dir=str(NLTK_DIR))
nltk.download('punkt', download_dir=str(NLTK_DIR))
nltk.download('averaged_perceptron_tagger_eng', download_dir=str(NLTK_DIR))
nltk.download('wordnet', download_dir=str(NLTK_DIR))
'''

'''
å†™å…¥è‡ªå®šä¹‰åœç”¨è¯æ–‡ä»¶è·¯å¾„ï¼Œmy_stopwords.txtæ–‡ä»¶æ”¾åœ¨äº†lab1ç›®å½•ä¸‹
éœ€è¦è‡ªå·±ä¿®æ”¹æ–‡ä»¶my_stopwords.txtçš„ä½ç½®ï¼Œè¿˜æœ‰ä¸‹é¢çš„my_stopwords.txtçš„è·¯å¾„
æœ€åèƒ½ç”¨å°±è¡ŒğŸ˜
'''
nltk_my_stopwords = BASE_DIR/"Lab1/nltk_dir/my_stopwords.txt"


with open(nltk_my_stopwords, "r", encoding="utf-8") as f:
    custom_stopwords = {line.strip() for line in f if line.strip()}
#å–è¯çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œå…¶æ„ä¹‰å¯ä»¥è®¿é—®regex101.comæŸ¥çœ‹
tokenizer = RegexpTokenizer(r"[a-zA-Z]+(?:-[a-zA-Z]+)*")
#åŠ è½½åœç”¨è¯
stop_words = set(stopwords.words("english"))
stop_words.update(custom_stopwords)

lemmatizer = WordNetLemmatizer()

txt_files = [f for f in os.listdir(input_folder) if f.endswith(".txt")]

for filename in tqdm(txt_files, desc="Tokenizing files", ncols=80):
    
    filepath = os.path.join(input_folder, filename)
    with open(filepath, "r", encoding="utf-8") as f:
        text = f.read().lower()
    #åˆ†è¯
    tokens = tokenizer.tokenize(text)
    #å»åœç”¨è¯
    filtered_tokens = [t for t in tokens if t not in stop_words]
    #ç»™å•è¯è´´è¯æ€§æ ‡ç­¾å¹¶è¿˜åŸ
    pos_tags = pos_tag(filtered_tokens)
    lemmatized_tokens = [
        lemmatizer.lemmatize(word,get_wordnet_pos(pos))
        for word,pos in pos_tags
    ]

    output_path = os.path.join(output_folder, filename)
    with open(output_path, "w", encoding="utf-8") as out:
        out.write(" ".join(lemmatized_tokens))

print("\nAll documents have been tokenized and saved in:", output_folder)